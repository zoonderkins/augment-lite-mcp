defaults:
  temperature: 0.3
  top_p: 1.0
  max_output_tokens: 8192
  timeout_ms: 60000

providers:
  # ============================================================
  # GLM-4.7 原厂 (智谱 - OpenAI 兼容格式)
  # ============================================================
  # Z.AI Coding Plan endpoint (OpenAI-compatible)
  # 需设置: GLM_API_KEY (从 z.ai 获取)
  # Endpoint: POST /chat/completions
  glm-4.7:
    base_url: "https://api.z.ai/api/coding/paas/v4"
    api_key_env: "GLM_API_KEY"
    type: "openai-compatible"
    model_id: "GLM-4.7"

  # ============================================================
  # GLM 本地代理 (claude-code-proxy)
  # ============================================================
  # 需先启动: start-all-proxies.sh (端口 8082)
  # 设置: GLM_LOCAL_BASE_URL (可选覆盖)
  glm-local:
    base_url_env: "GLM_LOCAL_BASE_URL"
    api_key_env: "GLM_LOCAL_API_KEY"
    type: "openai-compatible"
    model_id_env: "GLM_LOCAL_MODEL_ID"

  # ============================================================
  # MiniMax-M2.1 原厂 (OpenAI 兼容格式)
  # ============================================================
  # 直连原厂 API，使用 OpenAI Chat Completions 格式
  # 需设置: MINIMAX_API_KEY (从 minimax.io 获取)
  # Endpoint: POST /v1/chat/completions
  minimax-m2.1:
    base_url: "https://api.minimax.io/v1"
    api_key_env: "MINIMAX_API_KEY"
    type: "openai-compatible"
    model_id: "MiniMax-M2.1"

  # ============================================================
  # MiniMax 本地代理 (claude-code-proxy)
  # ============================================================
  # 需先启动: start-all-proxies.sh (端口 8083)
  # 设置: MINIMAX_LOCAL_BASE_URL (可选覆盖)
  minimax-local:
    base_url_env: "MINIMAX_LOCAL_BASE_URL"
    api_key_env: "MINIMAX_LOCAL_API_KEY"
    type: "openai-compatible"
    model_id_env: "MINIMAX_LOCAL_MODEL_ID"

  # ============================================================
  # Gemini Provider (已禁用)
  # ============================================================
  # 如需启用，取消注释:
  # gemini-2.5-flash:
  #   base_url: "https://generativelanguage.googleapis.com/v1beta/openai"
  #   api_key_env: "GEMINI_API_KEY"
  #   type: "openai-compatible"
  #   model_id: "gemini-2.5-flash"
  #
  # gemini-local:
  #   base_url_env: "GEMINI_LOCAL_BASE_URL"
  #   api_key_env: "GEMINI_LOCAL_API_KEY"
  #   type: "openai-compatible"
  #   model_id_env: "GEMINI_LOCAL_MODEL_ID"

  # ============================================================
  # Requesty.ai providers (聚合服务)
  # ============================================================
  requesty-gpt5:
    base_url: "https://router.requesty.ai/v1"
    api_key_env: "REQUESTY_API_KEY"
    type: "openai-compatible"
    model_id: "openai/gpt-5-chat"
  requesty-gemini:
    base_url: "https://router.requesty.ai/v1"
    api_key_env: "REQUESTY_API_KEY"
    type: "openai-compatible"
    model_id: "google/gemini-2.5-flash"
  requesty-qwen3-coder:
    base_url: "https://router.requesty.ai/v1"
    api_key_env: "REQUESTY_API_KEY"
    type: "openai-compatible"
    model_id: "alibaba/qwen3-coder-plus"
  requesty-claude-haiku:
    base_url: "https://router.requesty.ai/v1"
    api_key_env: "REQUESTY_API_KEY"
    type: "openai-compatible"
    model_id: "anthropic/claude-haiku-4-5"

# ============================================================
# Routes (默认使用原厂 providers)
# ============================================================
# 如需切换到本地代理，修改 model 为 glm-local / minimax-local
routes:
  small-fast:
    model: "minimax-m2.1"
    max_output_tokens: 2048
  reason-large:
    model: "glm-4.7"
    max_output_tokens: 8192
  general:
    model: "glm-4.7"
    max_output_tokens: 4096
  big-mid:
    model: "glm-4.7"
    max_output_tokens: 8192
  long-context:
    model: "glm-4.7"              # 使用原厂 GLM-4.7 (200K context)
    max_output_tokens: 8192
  ultra-long-context:
    model: "glm-4.7"              # 使用原厂 GLM-4.7 (200K context)
    max_output_tokens: 16384
  fast-reasoning:
    model: "minimax-m2.1"
    max_output_tokens: 4096

routing_thresholds:
  small_max_tokens: 200000
  big_mid_max_tokens: 400000
  long_context_max_tokens: 1000000
