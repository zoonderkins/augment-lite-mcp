defaults:
  temperature: 0.3
  top_p: 1.0
  max_output_tokens: 8192
  timeout_ms: 60000

providers:
  # Local proxies (you keep them)
  kimi-k2-0905:
    base_url: "http://127.0.0.1:8081/v1"
    api_key_env: "KIMI_LOCAL_KEY"
    type: "openai-compatible"
    model_id: "groq/moonshotai/kimi-k2-instruct-0905"  # 傳給 proxy 的模型名稱
  glm-4.6:
    base_url: "http://127.0.0.1:8082/v1"
    api_key_env: "GLM_LOCAL_KEY"
    type: "openai-compatible"
    model_id: "zai/glm-4.6"  # 傳給 proxy 的模型名稱
  minimaxi-m2:
    base_url: "http://127.0.0.1:8083/v1"
    api_key_env: "MINIMAXI_LOCAL_KEY"
    type: "openai-compatible"
    model_id: "minimaxi/minimax-m2"  # 傳給 proxy 的模型名稱
  gemini-local:
    base_url: "http://127.0.0.1:8084/v1"
    api_key_env: "GEMINI_LOCAL_KEY"  # Port 8084 proxy 使用 "dummy" 即可
    type: "openai-compatible"
    model_id: "google/gemini-2.5-flash"  # 傳給 proxy 的模型名稱

  # Requesty.ai providers (fill model_id from their catalog)
  requesty-gpt5:
    base_url: "https://router.requesty.ai/v1"
    api_key_env: "REQUESTY_API_KEY"
    type: "openai-compatible"
    model_id: "openai/gpt-5-chat"              # adjust per availability
  requesty-gemini:
    base_url: "https://router.requesty.ai/v1"
    api_key_env: "REQUESTY_API_KEY"
    type: "openai-compatible"
    model_id: "google/gemini-2.5-flash"   # or gemini-2.5-*
  requesty-qwen3-coder:
    base_url: "https://router.requesty.ai/v1"
    api_key_env: "REQUESTY_API_KEY"
    type: "openai-compatible"
    model_id: "alibaba/qwen3-coder-plus"  # 1,048,576 tokens context
  requesty-claude-haiku:
    base_url: "https://router.requesty.ai/v1"
    api_key_env: "REQUESTY_API_KEY"
    type: "openai-compatible"
    model_id: "anthropic/claude-haiku-4-5"  # 200,000 tokens context

routes:
  small-fast:
    model: "minimaxi-m2"
    max_output_tokens: 2048       # 快速查詢，短回答
  reason-large:
    model: "requesty-qwen3-coder"
    max_output_tokens: 8192       # 推理任務，中等長度
  general:
    model: "glm-4.6"
    max_output_tokens: 4096       # 一般查詢，中等長度
  big-mid:
    model: "requesty-gpt5"        # 200k~400k
    max_output_tokens: 8192       # 大型任務，長回答
  long-context:
    model: "gemini-local"         # 400k~1M (使用本地 Port 8084 Gemini proxy)
    max_output_tokens: 8192       # 長上下文，長回答
  ultra-long-context:
    model: "requesty-qwen3-coder" # >1M (up to 1,048,576 tokens)
    max_output_tokens: 16384      # 超長上下文，完整回答
  fast-reasoning:
    model: "requesty-claude-haiku" # Claude Haiku 4.5 (fast + smart)
    max_output_tokens: 4096       # 快速推理，中等長度

routing_thresholds:
  small_max_tokens: 200000        # ≤200k → small-fast/reason-large/general
  big_mid_max_tokens: 400000      # 200k~400k → big-mid
  long_context_max_tokens: 1000000 # 400k~1M → long-context
  # >1M → ultra-long-context