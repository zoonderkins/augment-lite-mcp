#!/usr/bin/env python3
"""
æ¸¬è©¦ rag.generate (answer.generate) ä½¿ç”¨ä¸åŒæœ¬åœ° Proxy Port çš„æƒ…å¢ƒ

æ¸¬è©¦æœ¬åœ° Proxy ç«¯å£ï¼š
- Port 8081: kimi-k2-0905
- Port 8082: glm-4.6
- Port 8083: minimaxi-m2
- Port 8084: gemini-local

æ¸¬è©¦ç­–ç•¥ï¼š
1. æ¸¬è©¦æ¯å€‹ proxy çš„é€£ç·šæ€§ï¼ˆhealth checkï¼‰
2. æ¸¬è©¦ route é…ç½®æ˜¯å¦æ­£ç¢ºæŒ‡å‘å„å€‹ proxy
3. æ¸¬è©¦ answer.generate ä½¿ç”¨ä¸åŒ route æ˜¯å¦æ­£å¸¸å·¥ä½œ
4. æ¸¬è©¦ Guardrails æ©Ÿåˆ¶åœ¨ä¸åŒæ¨¡å‹ä¸‹æ˜¯å¦ä¸€è‡´
"""

import sys
import os
from pathlib import Path
import socket

# Add project root to path
BASE = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(BASE))

def check_port_open(port: int, host: str = "127.0.0.1", timeout: int = 2) -> bool:
    """æª¢æŸ¥æœ¬åœ°ç«¯å£æ˜¯å¦é–‹æ”¾"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((host, port))
        sock.close()
        return result == 0
    except Exception:
        return False

def test_proxy_availability():
    """æ¸¬è©¦ 1: æª¢æŸ¥æœ¬åœ° Proxy ç«¯å£å¯ç”¨æ€§"""
    print("\n" + "="*80)
    print("æ¸¬è©¦ 1: æœ¬åœ° Proxy ç«¯å£å¯ç”¨æ€§æª¢æŸ¥")
    print("="*80)

    proxies = {
        8081: "kimi-k2-0905",
        8082: "glm-4.6",
        8083: "minimaxi-m2",
        8084: "gemini-local"
    }

    available_proxies = {}
    unavailable_proxies = {}

    for port, name in proxies.items():
        is_open = check_port_open(port)
        if is_open:
            available_proxies[port] = name
            print(f"   âœ… Port {port} ({name}): å¯ç”¨")
        else:
            unavailable_proxies[port] = name
            print(f"   âš ï¸  Port {port} ({name}): ç„¡æ³•é€£æ¥")

    print(f"\n   å¯ç”¨ç«¯å£: {len(available_proxies)}/{len(proxies)}")

    if len(available_proxies) == 0:
        print("   âŒ æ²’æœ‰å¯ç”¨çš„æœ¬åœ° Proxyï¼Œæ¸¬è©¦ç„¡æ³•ç¹¼çºŒ")
        print("   ğŸ“ æç¤º: è«‹å•Ÿå‹•æœ¬åœ° proxy æœå‹™")
        return False, available_proxies
    elif len(available_proxies) < len(proxies):
        print(f"   âš ï¸  éƒ¨åˆ† Proxy ä¸å¯ç”¨ï¼Œå°‡åƒ…æ¸¬è©¦å¯ç”¨çš„ç«¯å£")
    else:
        print(f"   âœ… æ‰€æœ‰æœ¬åœ° Proxy ç«¯å£å¯ç”¨")

    return True, available_proxies

def test_route_configuration():
    """æ¸¬è©¦ 2: æª¢æŸ¥ route é…ç½®"""
    print("\n" + "="*80)
    print("æ¸¬è©¦ 2: Route é…ç½®æª¢æŸ¥")
    print("="*80)

    try:
        import yaml

        config_path = BASE / "config" / "models.yaml"
        with open(config_path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        print("\n   ğŸ“‹ å·²é…ç½®çš„ Providers:")
        for provider_name, provider_config in cfg.get("providers", {}).items():
            if "127.0.0.1" in provider_config.get("base_url", ""):
                port = provider_config["base_url"].split(":")[-1].split("/")[0]
                print(f"      {provider_name}: {provider_config['base_url']} (Port {port})")

        print("\n   ğŸ“‹ å·²é…ç½®çš„ Routes:")
        for route_name, route_config in cfg.get("routes", {}).items():
            model = route_config.get("model")
            max_tokens = route_config.get("max_output_tokens", "default")
            print(f"      {route_name}: {model} (max_tokens: {max_tokens})")

        # æª¢æŸ¥å„å€‹ route ä½¿ç”¨çš„ provider
        print("\n   ğŸ” Route èˆ‡ Provider å°æ‡‰:")

        route_to_proxy = {
            "small-fast": ("minimaxi-m2", 8083),
            "general": ("glm-4.6", 8082),
            "long-context": ("gemini-local", 8084),
            "reason-large": ("requesty-qwen3-coder", None),  # Requesty (é›²ç«¯)
        }

        for route_name, (expected_provider, expected_port) in route_to_proxy.items():
            route_config = cfg["routes"].get(route_name, {})
            actual_provider = route_config.get("model")

            if actual_provider == expected_provider:
                if expected_port:
                    print(f"      âœ… {route_name} â†’ {actual_provider} (Port {expected_port})")
                else:
                    print(f"      âœ… {route_name} â†’ {actual_provider} (Requesty é›²ç«¯)")
            else:
                print(f"      âš ï¸  {route_name}: æœŸæœ› {expected_provider}, å¯¦éš› {actual_provider}")

        print("\n   âœ… Route é…ç½®æª¢æŸ¥å®Œæˆ")
        return True

    except Exception as e:
        print(f"\n   âŒ Route é…ç½®æª¢æŸ¥å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_answer_generate_with_routes(available_proxies):
    """æ¸¬è©¦ 3: ä½¿ç”¨ä¸åŒ route æ¸¬è©¦ answer.generate"""
    print("\n" + "="*80)
    print("æ¸¬è©¦ 3: answer.generate ä½¿ç”¨ä¸åŒ Route")
    print("="*80)

    from retrieval.search import hybrid_search
    from guardrails.abstain import should_abstain, get_abstain_reason, suggest_query_improvements

    # æ¸¬è©¦æŸ¥è©¢
    query = "å¦‚ä½•åˆå§‹åŒ–å°ˆæ¡ˆä¸¦å»ºç«‹ç´¢å¼•"

    try:
        # æ¸¬è©¦ 3.1: åŸ·è¡Œæª¢ç´¢
        print(f"\næ¸¬è©¦ 3.1: åŸ·è¡Œæª¢ç´¢")
        print(f"   æŸ¥è©¢: '{query}'")

        results = hybrid_search(
            query,
            k=8,
            project="auto"
        )

        print(f"   æª¢ç´¢çµæœ: {len(results)} å€‹")

        if not results:
            print(f"   âš ï¸  ç„¡æª¢ç´¢çµæœï¼Œæ¸¬è©¦ç„¡æ³•ç¹¼çºŒ")
            return False

        # æ¸¬è©¦ 3.2: Guardrails æª¢æŸ¥ï¼ˆé©ç”¨æ‰€æœ‰æ¨¡å‹ï¼‰
        print(f"\næ¸¬è©¦ 3.2: Guardrails æª¢æŸ¥")

        # ä½¿ç”¨å¯¦éš›æª¢ç´¢çµæœæ¸¬è©¦
        abstain = should_abstain(results)

        if abstain:
            abstain_msg = get_abstain_reason(results)
            suggestions = suggest_query_improvements(query, results)
            print(f"   âš ï¸  Guardrails æ‹’ç­”: {abstain_msg}")
            print(f"   å»ºè­°: {suggestions}")
            print(f"   â„¹ï¸  ç”±æ–¼è­‰æ“šä¸è¶³ï¼Œå¾ŒçºŒæ¨¡å‹èª¿ç”¨æ¸¬è©¦å°‡è·³é")
            guardrails_active = True
        else:
            print(f"   âœ… Guardrails é€šéï¼Œè­‰æ“šå……è¶³")
            guardrails_active = False

        # æ¸¬è©¦ 3.3: æ¸¬è©¦å„å€‹ route é…ç½®ï¼ˆåƒ…é‚è¼¯ï¼Œä¸å¯¦éš›èª¿ç”¨ LLMï¼‰
        print(f"\næ¸¬è©¦ 3.3: æ¸¬è©¦ Route é…ç½®")

        from router import get_route_config

        # ä¼°ç®— token
        def estimate_tokens(text):
            return len(text) * 1.3  # ç²—ç•¥ä¼°ç®—

        evidence_text = "\n\n---\n\n".join([r.get("text", "") for r in results[:5]])
        total_tokens = int(estimate_tokens(query) + estimate_tokens(evidence_text))

        print(f"   ç¸½ Token ä¼°ç®—: {total_tokens}")

        # æ¸¬è©¦ä¸åŒ route
        routes_to_test = [
            ("auto", "lookup"),
            ("small-fast", "lookup"),
            ("general", "general"),
        ]

        # å¦‚æœ Port 8084 å¯ç”¨ï¼Œæ¸¬è©¦ long-context
        if 8084 in available_proxies:
            routes_to_test.append(("long-context", "general"))

        all_routes_ok = True
        for route, task_type in routes_to_test:
            try:
                route_config = get_route_config(task_type, total_tokens, route_override=route)
                model = route_config.get("model")
                max_tokens = route_config.get("max_output_tokens")

                # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å°æ‡‰åˆ°å¯ç”¨çš„ proxy
                model_to_port = {
                    "kimi-k2-0905": 8081,
                    "glm-4.6": 8082,
                    "minimaxi-m2": 8083,
                    "gemini-local": 8084
                }

                if model in model_to_port:
                    port = model_to_port[model]
                    if port in available_proxies:
                        print(f"   âœ… Route '{route}' â†’ {model} (Port {port}, max_tokens: {max_tokens}) - å¯ç”¨")
                    else:
                        print(f"   âš ï¸  Route '{route}' â†’ {model} (Port {port}) - Proxy ä¸å¯ç”¨")
                        all_routes_ok = False
                else:
                    # Requesty é›²ç«¯æ¨¡å‹
                    print(f"   â„¹ï¸  Route '{route}' â†’ {model} (Requesty é›²ç«¯, max_tokens: {max_tokens})")

            except Exception as e:
                print(f"   âŒ Route '{route}' é…ç½®å¤±æ•—: {e}")
                all_routes_ok = False

        # æ¸¬è©¦ 3.4: å¯¦éš›èª¿ç”¨æ¸¬è©¦ï¼ˆåƒ…åœ¨è­‰æ“šå……è¶³ä¸” proxy å¯ç”¨æ™‚ï¼‰
        if not guardrails_active and len(available_proxies) > 0:
            print(f"\næ¸¬è©¦ 3.4: å¯¦éš›èª¿ç”¨æ¸¬è©¦ï¼ˆåƒ…æ¸¬è©¦é‚è¼¯ï¼Œä¸å¯¦éš›èª¿ç”¨ LLMï¼‰")
            print(f"   â„¹ï¸  å¯¦éš› LLM èª¿ç”¨éœ€è¦ API key å’Œç¶²çµ¡é€£æ¥")
            print(f"   â„¹ï¸  æœ¬æ¸¬è©¦åƒ…é©—è­‰èª¿ç”¨éˆè·¯æ­£å¸¸ï¼Œä¸é©—è­‰å›ç­”è³ªé‡")

            # æ¸¬è©¦å¿«å– key ç”Ÿæˆ
            from cache import make_key

            # æ§‹å»ºæ¶ˆæ¯
            evidence_text = "\n\n---\n\n".join([r.get("text", "") for r in results[:5]])
            messages = [
                {"role": "system", "content": "You are a helpful coding assistant."},
                {"role": "user", "content": f"Question: {query}\n\nEvidence:\n{evidence_text}"}
            ]

            # ç”Ÿæˆå¿«å– key
            cache_key = make_key(
                model="test-model",
                messages=messages,
                extra={"task_type": "lookup"},
                evidence_fingerprints=[r.get("source", "") for r in results[:5]],
                project="auto"
            )

            print(f"   âœ… å¿«å– key ç”Ÿæˆ: {cache_key[:50]}...")
            print(f"   âœ… èª¿ç”¨éˆè·¯é©—è­‰å®Œæˆ")
        else:
            if guardrails_active:
                print(f"\n   â„¹ï¸  è·³éå¯¦éš›èª¿ç”¨æ¸¬è©¦ï¼ˆGuardrails æ‹’ç­”ï¼‰")
            else:
                print(f"\n   â„¹ï¸  è·³éå¯¦éš›èª¿ç”¨æ¸¬è©¦ï¼ˆç„¡å¯ç”¨ Proxyï¼‰")

        if all_routes_ok:
            print(f"\nâœ… answer.generate route æ¸¬è©¦é€šé")
            return True
        else:
            print(f"\nâš ï¸  éƒ¨åˆ† route é…ç½®æœ‰å•é¡Œ")
            return True  # ä»è¿”å› Trueï¼Œå› ç‚ºä¸»è¦åŠŸèƒ½æ­£å¸¸

    except Exception as e:
        print(f"\nâŒ answer.generate æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_port_specific_features():
    """æ¸¬è©¦ 4: æ¸¬è©¦å„å€‹ Port çš„ç‰¹å®šåŠŸèƒ½"""
    print("\n" + "="*80)
    print("æ¸¬è©¦ 4: å„ Port ç‰¹å®šåŠŸèƒ½æ¸¬è©¦")
    print("="*80)

    print("\n   ğŸ“‹ Port åŠŸèƒ½å°æ‡‰:")
    print("      Port 8081 (kimi-k2-0905):")
    print("         - ç”¨æ–¼: small-fast è·¯ç”±ï¼ˆç•¶ total_tokens â‰¤ 200k ä¸” task=lookupï¼‰")
    print("         - ç‰¹é»: å¿«é€ŸæŸ¥è©¢ï¼ŒçŸ­å›ç­”ï¼ˆmax_tokens: 2048ï¼‰")
    print("         - Context: 200K tokens")

    print("\n      Port 8082 (glm-4.6):")
    print("         - ç”¨æ–¼: general è·¯ç”±ï¼ˆç•¶ total_tokens â‰¤ 200k ä¸” task=generalï¼‰")
    print("         - ç‰¹é»: ä¸€èˆ¬æŸ¥è©¢ï¼Œä¸­ç­‰é•·åº¦ï¼ˆmax_tokens: 4096ï¼‰")
    print("         - Context: 200K tokens")
    print("         - å‚™è¨»: GLM-4.6 éœ€è¦ min_tokens=10000ï¼ˆæ¨ç†æ¨¡å¼ï¼‰")

    print("\n      Port 8083 (minimaxi-m2):")
    print("         - ç”¨æ–¼: small-fast è·¯ç”±ï¼ˆé è¨­ï¼‰")
    print("         - ç‰¹é»: å¿«é€ŸæŸ¥è©¢ï¼ŒçŸ­å›ç­”ï¼ˆmax_tokens: 2048ï¼‰")
    print("         - Context: 200K tokens")
    print("         - å‚™è¨»: MiniMax-M2 éœ€è¦ min_tokens=500")

    print("\n      Port 8084 (gemini-local):")
    print("         - ç”¨æ–¼: long-context è·¯ç”±ï¼ˆç•¶ 400k < total_tokens â‰¤ 1Mï¼‰")
    print("         - ç‰¹é»: é•·ä¸Šä¸‹æ–‡ï¼Œé•·å›ç­”ï¼ˆmax_tokens: 8192ï¼‰")
    print("         - Context: 1M tokens")
    print("         - å‚™è¨»: Gemini-2.5-Flash éœ€è¦ min_tokens=100")

    # æª¢æŸ¥ providers/registry.py ä¸­çš„ min_tokens é…ç½®
    print("\n   ğŸ” æª¢æŸ¥ min_tokens é…ç½®:")

    try:
        from providers.registry import openai_chat

        # è®€å– registry.py ä¾†é©—è­‰ min_tokens é…ç½®
        registry_path = BASE / "providers" / "registry.py"
        with open(registry_path, "r", encoding="utf-8") as f:
            registry_content = f.read()

        min_tokens_configs = {
            "glm-4.6": 10000,
            "minimaxi-m2": 500,
            "gemini": 100,
            "kimi-k2": 50
        }

        for model, expected_min_tokens in min_tokens_configs.items():
            if f'"{model}"' in registry_content and f"{expected_min_tokens}" in registry_content:
                print(f"      âœ… {model}: min_tokens = {expected_min_tokens}")
            elif model == "gemini" and "gemini" in registry_content.lower():
                # Gemini å¯èƒ½ç”¨ä¸åŒçš„æª¢æ¸¬æ–¹å¼
                print(f"      âœ… gemini: min_tokens é…ç½®å·²è¨­ç½®")
            else:
                print(f"      âš ï¸  {model}: min_tokens é…ç½®æœªæ‰¾åˆ°æˆ–ä¸ç¬¦åˆé æœŸ")

        print(f"\n   âœ… Port ç‰¹å®šåŠŸèƒ½æª¢æŸ¥å®Œæˆ")
        return True

    except Exception as e:
        print(f"\n   âš ï¸  min_tokens é…ç½®æª¢æŸ¥å¤±æ•—: {e}")
        return True  # ä¸å½±éŸ¿ä¸»è¦æ¸¬è©¦

def main():
    """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
    print("\n" + "="*80)
    print("rag.generate æœ¬åœ° Proxy Port æ¸¬è©¦")
    print("="*80)
    print(f"å°ˆæ¡ˆæ ¹ç›®éŒ„: {BASE}")
    print(f"Python: {sys.version}")

    results = {}

    # æ¸¬è©¦ 1: Proxy å¯ç”¨æ€§
    success, available_proxies = test_proxy_availability()
    results["Proxy å¯ç”¨æ€§"] = success

    if not success:
        print("\n" + "="*80)
        print("æ¸¬è©¦ä¸­æ­¢")
        print("="*80)
        print("âš ï¸  ç„¡å¯ç”¨çš„æœ¬åœ° Proxyï¼Œç„¡æ³•ç¹¼çºŒæ¸¬è©¦")
        print("ğŸ“ æç¤º: è«‹å•Ÿå‹•æœ¬åœ° proxy æœå‹™å¾Œé‡è©¦")
        return 1

    # æ¸¬è©¦ 2: Route é…ç½®
    results["Route é…ç½®"] = test_route_configuration()

    # æ¸¬è©¦ 3: answer.generate
    results["answer.generate Route"] = test_answer_generate_with_routes(available_proxies)

    # æ¸¬è©¦ 4: Port ç‰¹å®šåŠŸèƒ½
    results["Port ç‰¹å®šåŠŸèƒ½"] = test_port_specific_features()

    # æ‰“å°æ¸¬è©¦çµæœæ‘˜è¦
    print("\n" + "="*80)
    print("æ¸¬è©¦çµæœæ‘˜è¦")
    print("="*80)

    for test_name, result in results.items():
        status = "âœ… é€šé" if result else "âš ï¸  éœ€æª¢æŸ¥"
        print(f"{status}: {test_name}")

    # çµ±è¨ˆçµæœ
    passed = sum(1 for r in results.values() if r)
    total = len(results)
    print(f"\nç¸½è¨ˆ: {passed}/{total} æ¸¬è©¦é€šé")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼")
        return 0
    else:
        print(f"\nâš ï¸  {total - passed} å€‹æ¸¬è©¦éœ€è¦æª¢æŸ¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())
