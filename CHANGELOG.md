# Changelog

All notable changes to augment-lite MCP server will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.3.3] - 2026-01-18

### âœ¨ New Features
- **OpenRouter Embedding API**: Added `EmbeddingProvider` class supporting API embeddings via OpenRouter
  - Primary: `qwen/qwen3-embedding-4b` (2560 dims)
  - Fallback: Local `sentence-transformers/all-MiniLM-L6-v2` (384 dims)
  - Config: `config/models.yaml` â†’ `embedding` section
- **CJK Tokenizer**: Added `_simple_tokenize()` for Chinese/Japanese/Korean document chunking
- **Extended File Types**: Expanded to 70+ supported file extensions (CODE_EXTS + DOC_EXTS)

### ğŸ”§ Improvements
- **TopK Over-fetch**: Increased from `kÃ—2` to `kÃ—3` for larger candidate pool
- **Per-file Dedup**: Changed from limit=1 to limit=2 (better recall, less aggressive)
- **Safe Source Parsing**: Regex `r":(?:chunk)?\d+$"` handles Windows paths, URLs safely
- **Dimension Guard**: Fail-fast check with provider/model info in error messages
- **Single-vector Shape**: Handle `(D,)` â†’ `(1, D)` with ndim guard

### ğŸ“š Documentation
- **Mermaid Diagrams**: Added system architecture and hybrid search flow diagrams
- **Technical Parameters**: Updated table with kÃ—3, per-file limit=2, 2560 dims
- **File Types List**: Added collapsible section with 70+ supported extensions

---

## [1.3.2] - 2026-01-16

### âœ¨ New Features
- **Unified Search Tools**: Added 3 new tools for dual-MCP orchestration
  - `answer.accumulated`: Multi-round evidence accumulation with automatic query decomposition
  - `answer.unified`: Execution plan generator for auggie-mcp + augment-lite coordination
  - `dual.search`: Combined search wrapper with auggie invocation hint
- **Auto-Rebuild Stale Index**: `dual.search` auto-detects when auggie finds files missing from augment-lite (>50%), triggers `incremental_index` rebuild and re-searches
  - New parameter: `auto_rebuild` (default: true)
  - Returns: `index_rebuilt`, `rebuild_info` fields

### ğŸ”§ Bug Fixes
- **Fixed MiniMax config**: Changed to OpenAI-compatible format (`type: "openai-compatible"`)
- **Fixed cache import**: Corrected `cache_get/cache_set` to `get/set` in accumulated_answer.py
- **Fixed __pycache__ stale bytecode**: Auto-clear `__pycache__` on MCP server startup (excludes `.venv`)
- **Fixed index.rebuild subprocess**: Use `.venv/bin/python` instead of system Python to ensure duckdb/faiss available
- **Improved index.rebuild errors**: Return stderr/stdout/cmd in error response for debugging

### ğŸ“š Documentation
- Added Section 8 (Unified Search) to README with Auto-Rebuild documentation
- Updated SERVER_INSTRUCTIONS with tool selection guide
- Total tools: 31

---

## [1.3.1] - 2026-01-14

### ğŸ”§ Bug Fixes
- **Fixed `resolve_auto_project()` inconsistency**: `project_status` now correctly reports memory keys count
  - Root cause: Multiple modules had duplicate `_get_active_project()` implementations that only checked `active=True`
  - Fixed: All modules now use unified `resolve_auto_project()` with smart directory matching
  - Affected files: `cache.py`, `semantic_cache.py`, `retrieval/search.py`, `retrieval/vector_search.py`, `retrieval/build_vector_index.py`, `utils/project_utils.py`

### ğŸ“š Documentation
- Updated README roadmap with v1.4.0/v1.5.0 plans (modify symbol tools, LSP bridge)

---

## [1.3.0] - 2025-01-14

### ğŸ”§ Bug Fixes
- **Fixed `sys` import bug**: `rag_search` with `auto_index=true` no longer crashes due to missing `sys` import
- **Fixed incremental_indexer**: Added missing functions (`load_gitignore`, `should_skip_file`, `parse_file_with_tree_sitter`) to `build_index.py`
- **Fixed setup scripts**: `setup_new_machine.sh` and `manage.sh` now correctly detect åŸå»  API vs local proxy mode

### âœ¨ New Features
- **AUTO-INIT workflow**: Projects auto-initialize when running `rag_search` without manual `project.init`
- **auggie-mcp collaboration modes**: Added Mode A/B/C documentation for MCP server coordination
- **Serena-style memory patterns**: Proactive memory system with standard keys (`project_overview`, `code_style`, etc.)

### ğŸ“š Documentation
- Added BM25+Vector technical architecture diagram to README
- Added execution logic flow diagram (Auto-Init â†’ Auto-Index â†’ Search)
- Added auggie-mcp integration section with collaboration modes

---

## [1.2.0] - 2025-01-13

### ğŸ›¡ï¸ Modern Guardrails Module
- Added configurable guardrails for MCP tool safety
- Implemented per-tool enable/disable settings
- Added audit logging for tool invocations

---

## [1.0.0] - 2025-11-10

### ğŸ‰ First Stable Release

augment-lite-mcp v1.0.0 æ˜¯ç¬¬ä¸€å€‹ç©©å®šç‰ˆæœ¬ï¼Œæ•´åˆäº† 0.x ç³»åˆ—æ‰€æœ‰åŠŸèƒ½ä¸¦ä¿®å¾©äº†é—œéµ bugã€‚

---

## ğŸš€ Major Features

### 1. Auto-Incremental Indexing
**Zero-Maintenance Search Experience** (acemcp-inspired)

- âœ… **è‡ªå‹•è®Šæ›´æª¢æ¸¬**: æœç´¢å‰è‡ªå‹•æª¢æ¸¬ä¸¦ç´¢å¼•æ–‡ä»¶è®Šæ›´
- âœ… **å¢é‡æ›´æ–°**: åªè™•ç†è®Šæ›´æ–‡ä»¶ï¼Œä¸é‡å»ºæ•´å€‹ç´¢å¼•
- âœ… **é€æ˜æ“ä½œ**: ç”¨æˆ¶ç„¡éœ€æ‰‹å‹•åŸ·è¡Œ `project.init` æˆ– `index.rebuild`
- âœ… **æ™ºèƒ½æª¢æ¸¬**: ä½¿ç”¨ mtimeã€æ–‡ä»¶å¤§å°å’Œ MD5 hash ç²¾ç¢ºæª¢æ¸¬è®Šæ›´
- âœ… **ç‹€æ…‹æŒä¹…åŒ–**: ç´¢å¼•ç‹€æ…‹å­˜å„²æ–¼ `data/index_state_{project}.json`

**Implementation**:
- New module: `retrieval/incremental_indexer.py`
- Integrated into `rag.search` tool with `auto_index` parameter (default: `True`)

**Performance**:
- ç·¨è¼¯ 1 å€‹æ–‡ä»¶: 60x faster (30s â†’ 0.5s)
- ç·¨è¼¯ 5 å€‹æ–‡ä»¶: 25x faster (30s â†’ 1.2s)
- æ–°å¢ 10 å€‹æ–‡ä»¶: 12x faster (30s â†’ 2.5s)

### 2. FastAPI Web UI
**Professional Management Interface**

- âœ… **å¯¦æ™‚æ—¥èªŒæµ**: WebSocket å¯¦æ™‚æ—¥èªŒæŸ¥çœ‹
- âœ… **æœç´¢æ¸¬è©¦**: äº¤äº’å¼æœç´¢ç•Œé¢
- âœ… **å°ˆæ¡ˆå„€è¡¨æ¿**: æŸ¥çœ‹æ‰€æœ‰å·²ç´¢å¼•å°ˆæ¡ˆ
- âœ… **ç¾ä»£åŒ– UI**: éŸ¿æ‡‰å¼æ·±è‰²ä¸»é¡Œ

**Stack**:
- FastAPI 0.121.1, Uvicorn 0.34.0, WebSockets 14.1

**Usage**:
```bash
cd web_ui && ./start.sh  # http://localhost:8080
```

### 3. Dual-Layer Retrieval Architecture
**High-Quality, Low-Cost Code Search**

- **Layer 1: Local Vector Embeddings**
  - Model: sentence-transformers/all-MiniLM-L6-v2 (384 dims)
  - Engine: PyTorch CPU (local, free)
  - Speed: ~50ms per search
  - BM25 + Vector hybrid search with score fusion

- **Layer 2: Remote LLM Re-ranking**
  - Model: Gemini 2.5 Flash (via local proxy port 8084)
  - Cost: ~$0.00005 per query
  - Speed: ~1s per search
  - Smart filtering with model-specific system prompts

**Results**:
- Accuracy: 85% (vs 70% pure local, 90% pure LLM)
- Cost: ~$0.00005 per query (vs $0.05 pure LLM)
- Latency: ~1.05s total

### 4. Multi-Project Management
**Flexible Project Organization**

- âœ… **ä¸‰ç¨®æŒ‡å®šæ–¹å¼**: åç¨± / ID (8 å­—å…ƒ) / auto (è‡ªå‹•åµæ¸¬)
- âœ… **å¿«é€Ÿåˆ‡æ›**: <1 ç§’åˆ‡æ›å°ˆæ¡ˆï¼Œç„¡éœ€é‡å»ºç´¢å¼•
- âœ… **å·¥ä½œç›®éŒ„æ„ŸçŸ¥**: MCP è‡ªå‹•ä½¿ç”¨ Claude Code ç•¶å‰å·¥ä½œå°ˆæ¡ˆ
- âœ… **å°ˆæ¡ˆéš”é›¢**: ç¨ç«‹çš„ç´¢å¼•ã€å¿«å–å’Œè¨˜æ†¶é«”

**CLI Management**:
```bash
./scripts/manage.sh add auto .         # è‡ªå‹•åµæ¸¬ä¸¦æ·»åŠ ç•¶å‰å°ˆæ¡ˆ
./scripts/manage.sh list                # åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆ
./scripts/manage.sh rebuild myproject   # é‡å»ºæŒ‡å®šå°ˆæ¡ˆç´¢å¼•
```

### 5. Advanced Caching System
**Three-Layer Cache for Performance**

- **Layer 1: Exact Cache** (SQLite)
  - ç²¾ç¢ºåŒ¹é…æŸ¥è©¢çµæœ
  - TTL: 1 hour

- **Layer 2: Semantic Cache** (FAISS)
  - å‘é‡ç›¸ä¼¼åº¦åŒ¹é… (threshold: 0.95)
  - 20% cache hit rate improvement

- **Layer 3: Provider Cache** (Requesty/Local Proxy)
  - 90% queries < 100ms response time

### 6. Long-Term Memory & Task Management

**Memory System**:
- SQLite key-value storage
- Global / Project scope isolation
- MCP tools: `memory.get`, `memory.set`, `memory.delete`

**Task System**:
- Structured task tracking with status (pending/in_progress/done/cancelled)
- Priority levels and subtasks support
- Resume mechanism for interrupted tasks
- MCP tools: `task.add`, `task.list`, `task.update`, `task.current`

### 7. MCP Protocol Compliance

**22 MCP Tools**:
- RAG search with auto-indexing
- Project management (init, status, rebuild)
- Cache management (clear, status)
- Memory operations (get, set, delete, list)
- Task management (add, list, update, get, delete, resume, current, stats)
- Answer generation with citations

**AI Auto-Discovery**:
- Server-level instructions guide AI on when to use each tool
- MCP resources expose indexed projects and memory
- Proactive usage patterns for zero-configuration experience

---

## âœ¨ Enhancements

### Model Support
- âœ… Gemini 2.5 Flash via local proxy (port 8084)
- âœ… Kimi, GLM, MiniMax via local proxies (ports 8081-8083)
- âœ… 300+ models via Requesty.ai aggregation
- âœ… Model-specific system prompts optimization
- âœ… Automatic min_tokens protection to avoid truncation

### System Prompts Customization
- âœ… `config/system_prompts.yaml` for per-model prompts
- âœ… Gemini: ç°¡æ½”ç‰ˆ (50 å­—ä»¥å…§)
- âœ… Claude: è©³ç´°ç‰ˆ (100+ å­—)
- âœ… Qwen: æ·±åº¦åˆ†æç‰ˆ

### Dynamic Token Limits
- âœ… Auto-adjust output tokens based on route (2048-16384)
- âœ… Prevents finish_reason="length" errors

### Guardrails
- âœ… Enforce evidence citation in answers
- âœ… Refuse to answer when evidence is insufficient
- âœ… Always provide source file references

---

## ğŸ› Bug Fixes

### Vector Index Auto Mode (v1.0.0)
- **Problem**: `./scripts/manage.sh add auto .` passed "auto" instead of resolved project name to `build_vector_index.py`
- **Symptom**: Vector index failed with "Chunks file not found: chunks_auto.jsonl"
- **Fix**: Added `resolve_project_name()` call in `add_project()` and `rebuild_project()` functions
- **Files**: `scripts/manage.sh` (line 187-188, 305-306)
- **Doc**: `docs/bugfixes/BUGFIX_VECTOR_INDEX_AUTO_MODE.md`

### Auto Mode Project Resolution (v0.6.0)
- Fixed MCP API "auto" mode resolution across all tools
- Added `resolve_project_name()` utility function
- Unified "auto" mode handling in MCP bridge

### Gitignore Filtering (v0.6.0)
- Fixed index to properly respect `.gitignore` rules
- Excluded `node_modules/`, `.git/`, build artifacts

### MCP API Error Handling (v0.5.1)
- Improved error messages for missing dependencies
- Better fallback when vector search unavailable

---

## ğŸ“ Documentation

### User Documentation
- `docs/guides/`: Comprehensive usage guides
  - MCP Setup, Multi-Project, Vector Search, Cache, Memory, Tasks
- `docs/features/`: Feature explanations
- `docs/core/`: Architecture and technical overview
  - **NEW**: `COMPARISON.md` - ç«¶å“æ¯”è¼ƒèˆ‡é¸å‹æŒ‡å—
    - vs Anthropic @modelcontextprotocol/context
    - vs acemcp (inspired by)
    - vs Augment Code (proprietary)
    - vs Qdrant/Weaviate (vector DBs)
    - Vector embedding models comparison (6 models)

### Developer Documentation
- `init/specs/`: Technical specifications
- `init/guidelines/`: Coding standards, naming conventions, documentation guide
- `init/workflows/`: Release, bugfix, and feature development workflows

### Configuration Examples
- **Claude MCP CLI**: One-command setup with environment variables
- **Manual JSON config**: Traditional config.json approach
- **Vector model switching**: 6 embedding models to choose from

---

## ğŸ¯ Performance Metrics

| Metric | Value |
|--------|-------|
| **Indexing Speed** | 1000+ files/sec (BM25) |
| **Search Latency** | ~1.05s (with LLM re-ranking) |
| **Cache Hit Rate** | 20% improvement with semantic cache |
| **Cost per Query** | ~$0.00005 (99.9% local processing) |
| **Accuracy** | 85% (hybrid search + LLM filtering) |

---

## âš ï¸ Breaking Changes

None. This is the first stable release.

---

## ğŸ™ Acknowledgments

### Inspiration & References

- **[acemcp](https://github.com/wxxedu/acemcp)** by @wxxedu
  - Auto-incremental indexing implementation
  - Zero-maintenance philosophy
  - Web UI design inspiration

- **[Augment Code](https://www.augmentcode.com/)** (Proprietary)
  - Context Engine architecture insights
  - Two-stage retrieval (local + remote) concept

- **[sentence-transformers](https://www.sbert.net/)** by Hugging Face
  - all-MiniLM-L6-v2 embedding model
  - Local, free, and high-quality embeddings

- **[Requesty.ai](https://requesty.ai/)**
  - Multi-model aggregation platform
  - 300+ model access with unified API

### Community Contributors

- Claude Code team for MCP protocol and development tools
- DuckDB team for embedded SQL database
- FAISS team (Meta) for vector similarity search
- FastAPI team for modern web framework

---

## ğŸ“¦ Installation

```bash
# Clone repository
git clone https://github.com/yourusername/augment-lite-mcp.git
cd augment-lite-mcp

# Install dependencies
python3 -m venv .venv
source .venv/bin/activate
uv pip install -r requirements-lock.txt

# Optional: Install vector search dependencies (~2GB)
bash scripts/install_vector_deps.sh

# Add project and build index
./scripts/manage.sh add auto .

# Configure MCP in Claude Code
# Add to ~/.claude/config.json:
{
  "mcpServers": {
    "augment-lite": {
      "command": "/path/to/.venv/bin/python",
      "args": ["/path/to/mcp_bridge_lazy.py"]
    }
  }
}
```

---

## ğŸ”— Links

- **Repository**: https://github.com/yourusername/augment-lite-mcp
- **Documentation**: See `docs/` directory
- **Issues**: https://github.com/yourusername/augment-lite-mcp/issues
- **License**: MIT

---

## ğŸ“… Release History

- **v1.0.0** (2025-11-10): First stable release
- **v0.7.0** (2024-11-09): Auto-incremental indexing, Web UI
- **v0.6.0** (2024-11-09): AI auto-discovery, MCP resources
- **v0.5.2** (2024-11-08): Token optimization, system prompts
- **v0.5.1** (2024-11-06): Documentation improvements
- **v0.5.0** (2024-11-03): Multi-project support
- **v0.4.0** (2024-10-28): Semantic cache, retry logic
- **v0.3.0** (2024-10-20): Vector search, hybrid retrieval

---

**[1.0.0]**: https://github.com/yourusername/augment-lite-mcp/releases/tag/v1.0.0
