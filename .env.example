# ============================================================
# augment-lite-mcp 環境變數配置
# ============================================================

# ============================================================
# 資料庫目錄
# ============================================================
# 所有資料檔案的儲存位置（索引、快取、記憶）
# 預設: ./data
AUGMENT_DB_DIR=./data

# ============================================================
# 本地 Proxy API Keys
# ============================================================
# 這些是本地 proxy 的 API keys（通常設為 "dummy" 即可）
# 本地 proxy 運行在 localhost:8081, 8082, 8083

# Kimi K2-0905 (localhost:8081)
KIMI_LOCAL_KEY=dummy

# GLM-4.6 (localhost:8082)
GLM_LOCAL_KEY=dummy

# Minimaxi M2 (localhost:8083)
MINIMAXI_LOCAL_KEY=dummy

# ============================================================
# Requesty.ai API Key
# ============================================================
# 用於訪問 Requesty.ai 聚合服務
# 支援的模型：
#   - openai/gpt-5-chat (200k tokens)
#   - google/gemini-2.5-flash (1M tokens)
#   - alibaba/qwen3-coder-plus (1,048,576 tokens)
#   - anthropic/claude-haiku-4-5 (200k tokens)
#
# 獲取 API Key: https://requesty.ai
REQUESTY_API_KEY=sk-your-requesty-api-key-here

# ============================================================
# 使用說明
# ============================================================
# 1. 複製此檔案為 .env
#    cp .env.example .env
#
# 2. 填入你的 REQUESTY_API_KEY
#
# 3. 如果使用本地 proxy，確保它們運行在正確的端口：
#    - Kimi: http://127.0.0.1:8081/v1
#    - GLM: http://127.0.0.1:8082/v1
#    - Minimaxi: http://127.0.0.1:8083/v1
#
# 4. 檢查本地 proxy 狀態：
#    ./scripts/manage.sh check-proxy
#
# 5. 如果不使用本地 proxy，可以在 config/models.yaml 中
#    將所有路由改為使用 Requesty 模型
# ============================================================